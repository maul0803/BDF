{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU & Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Modification: filtrage des valeurs égales à 0\n",
    "- ajout de .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/lustre/scratch/nlsas/home/ulc/cursos/curso341/miniconda3/envs/DaskOnRay38/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, BooleanType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task usage\n",
    "csv_directory_task_usage = \"/mnt/lustre/scratch/nlsas/home/ulc/cursos/curso341/Big_Data_Frameworks/data/task_usage/*.csv\"\n",
    "# Task event\n",
    "csv_directory_task_events = \"/mnt/lustre/scratch/nlsas/home/ulc/cursos/curso341/Big_Data_Frameworks/data/task_events/*.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/01 11:42:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Initialiser une session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Analyse Memory & CPU Usage (Jobs & Tasks)\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Définition du schéma pour les fichiers CSV\n",
    "task_usage_schema = StructType([\n",
    "    StructField(\"start time\", IntegerType(), True),\n",
    "    StructField(\"end time\", IntegerType(), True),\n",
    "    StructField(\"job ID\", IntegerType(), True),\n",
    "    StructField(\"task index\", IntegerType(), True),\n",
    "    StructField(\"machine ID\", IntegerType(), True),\n",
    "    StructField(\"CPU rate\", FloatType(), True),\n",
    "    StructField(\"canonical memory usage\", FloatType(), True),\n",
    "    StructField(\"assigned memory usage\", FloatType(), True),\n",
    "    StructField(\"unmapped page cache\", FloatType(), True),\n",
    "    StructField(\"total page cache\", FloatType(), True),\n",
    "    StructField(\"maximum memory usage\", FloatType(), True),\n",
    "    StructField(\"disk I/O time\", FloatType(), True),\n",
    "    StructField(\"local disk space usage\", FloatType(), True),\n",
    "    StructField(\"maximum CPU rate\", FloatType(), True),\n",
    "    StructField(\"maximum disk IO time\", FloatType(), True),\n",
    "    StructField(\"cycles per instruction\", FloatType(), True),\n",
    "    StructField(\"memory accesses per instruction\", FloatType(), True),\n",
    "    StructField(\"sample portion\", FloatType(), True),\n",
    "    StructField(\"aggregation type\", BooleanType(), True),\n",
    "    StructField(\"sampled CPU usage\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Lire les fichiers CSV en un DataFrame avec le schéma défini\n",
    "df_task_usage = spark.read.csv(csv_directory_task_usage, header=False, schema=task_usage_schema)\n",
    "\n",
    "#print(f\"Nombre total de lignes combinées (sans nettoyage) : {df_task_usage.count()}\") #1232799308\n",
    "# Filtrer pour supprimer les lignes où job ID est nul\n",
    "df_task_usage_cleaned = df_task_usage.filter(\n",
    "    F.col(\"job ID\").isNotNull() &\n",
    "    (F.col(\"canonical memory usage\") > 0) &\n",
    "    (F.col(\"sampled CPU usage\") > 0)\n",
    ")#.na.drop(subset=[\"canonical memory usage\", \"sampled CPU usage\"])\n",
    "\n",
    "#print(f\"Nombre total de lignes combinées (avec nettoyage) : {df_task_usage_cleaned.count()}\") #7901086\n",
    "\n",
    "# Sélectionner les colonnes pertinentes\n",
    "df_task_usage_cleaned_selected = df_task_usage_cleaned.select(\n",
    "    \"start time\", \"end time\", \"job ID\", \"task index\", \n",
    "    \"canonical memory usage\", \"sampled CPU usage\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----------+----------+----------+----------+--------------------+----------------+--------+------------------------------+------------------------+-------------------------------------+----------------------------+\n",
      "|timestamp|missing info|    job ID|task index|machine ID|event type|           user name|scheduling class|priority|resource request for CPU cores|resource request for RAM|resource request for local disk space|different-machine constraint|\n",
      "+---------+------------+----------+----------+----------+----------+--------------------+----------------+--------+------------------------------+------------------------+-------------------------------------+----------------------------+\n",
      "|     NULL|        NULL| 515042969|         0|      NULL|         5|/fk1fVcVxZ6iM6gHZ...|               2|       0|                       0.01562|                 0.01553|                             2.155E-4|                        NULL|\n",
      "|     NULL|        NULL| 515042969|         0|      NULL|         0|/fk1fVcVxZ6iM6gHZ...|               2|       0|                       0.01562|                 0.01553|                             2.155E-4|                        NULL|\n",
      "|     NULL|        NULL| 515042969|         0| 491535339|         1|/fk1fVcVxZ6iM6gHZ...|               2|       0|                       0.01562|                 0.01553|                             2.155E-4|                        NULL|\n",
      "|     NULL|        NULL|1412625411|        26|1436297839|         1|uHAVUszJvcY14C+f/...|               0|       0|                        0.0625|                 0.01746|                             0.003395|                        NULL|\n",
      "|     NULL|        NULL| 515042969|         7|      NULL|         5|/fk1fVcVxZ6iM6gHZ...|               2|       0|                       0.01562|                 0.01553|                             2.155E-4|                        NULL|\n",
      "|     NULL|        NULL| 515042969|         7|      NULL|         0|/fk1fVcVxZ6iM6gHZ...|               2|       0|                       0.01562|                 0.01553|                             2.155E-4|                        NULL|\n",
      "|     NULL|        NULL|1412625411|        28| 336055283|         1|uHAVUszJvcY14C+f/...|               0|       0|                        0.0625|                 0.01746|                             0.003395|                        NULL|\n",
      "|     NULL|        NULL|1412625411|        79|1429167889|         1|uHAVUszJvcY14C+f/...|               0|       0|                        0.0625|                 0.01746|                             0.003395|                        NULL|\n",
      "|     NULL|        NULL|1412625411|        81|   1094463|         1|uHAVUszJvcY14C+f/...|               0|       0|                        0.0625|                 0.01746|                             0.003395|                        NULL|\n",
      "|     NULL|        NULL|1412625411|        89| 294862969|         1|uHAVUszJvcY14C+f/...|               0|       0|                        0.0625|                 0.01746|                             0.003395|                        NULL|\n",
      "|     NULL|        NULL| 515042969|         2|      NULL|         1|/fk1fVcVxZ6iM6gHZ...|               2|       0|                       0.01562|                 0.01553|                             2.155E-4|                        NULL|\n",
      "|     NULL|        NULL| 515042969|         3|      NULL|         1|/fk1fVcVxZ6iM6gHZ...|               2|       0|                       0.01562|                 0.01553|                             2.155E-4|                        NULL|\n",
      "|     NULL|        NULL| 515042969|         4| 294847217|         1|/fk1fVcVxZ6iM6gHZ...|               2|       0|                       0.01562|                 0.01553|                             2.155E-4|                        NULL|\n",
      "|     NULL|        NULL| 515042969|         5|   3829174|         1|/fk1fVcVxZ6iM6gHZ...|               2|       0|                       0.01562|                 0.01553|                             2.155E-4|                        NULL|\n",
      "|     NULL|        NULL| 515042969|         6|   3829174|         1|/fk1fVcVxZ6iM6gHZ...|               2|       0|                       0.01562|                 0.01553|                             2.155E-4|                        NULL|\n",
      "|     NULL|        NULL|1863690462|         2| 288943802|         1|anXVp+pkoUreEzVWq...|               3|       0|                       0.03125|                0.006218|                             1.545E-4|                        NULL|\n",
      "|     NULL|        NULL| 515042969|         1|      NULL|         5|/fk1fVcVxZ6iM6gHZ...|               2|       0|                       0.01562|                 0.01553|                             2.155E-4|                        NULL|\n",
      "|     NULL|        NULL| 515042969|         1|      NULL|         0|/fk1fVcVxZ6iM6gHZ...|               2|       0|                       0.01562|                 0.01553|                             2.155E-4|                        NULL|\n",
      "|     NULL|        NULL| 515042969|         7|   1094553|         1|/fk1fVcVxZ6iM6gHZ...|               2|       0|                       0.01562|                 0.01553|                             2.155E-4|                        NULL|\n",
      "|     NULL|        NULL| 515042969|         8|   1094553|         1|/fk1fVcVxZ6iM6gHZ...|               2|       0|                       0.01562|                 0.01553|                             2.155E-4|                        NULL|\n",
      "+---------+------------+----------+----------+----------+----------+--------------------+----------------+--------+------------------------------+------------------------+-------------------------------------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:======================================================>(158 + 1) / 159]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+\n",
      "|    job ID|task index|priority|\n",
      "+----------+----------+--------+\n",
      "|1161381522|         0|       0|\n",
      "|1930851759|         4|       0|\n",
      "|1412625411|        13|       0|\n",
      "|1930851770|         0|       0|\n",
      "|1412625411|        43|       0|\n",
      "| 501114088|         0|       0|\n",
      "| 515042969|        19|       0|\n",
      "|1412625411|        35|       0|\n",
      "|1412625411|         7|       0|\n",
      "| 515042954|         6|       0|\n",
      "|1412625411|        27|       0|\n",
      "| 515042969|        15|       0|\n",
      "| 501114088|         3|       0|\n",
      "|1930851759|         6|       0|\n",
      "|1488089419|         0|       0|\n",
      "|1412625411|        47|       0|\n",
      "|1412625411|        59|       0|\n",
      "|1412625411|        77|       0|\n",
      "| 498778363|         0|       1|\n",
      "| 515042969|         9|       0|\n",
      "+----------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Définition du schéma pour les fichiers CSV\n",
    "task_events_schema = StructType([\n",
    "    StructField(\"timestamp\", IntegerType(), True),\n",
    "    StructField(\"missing info\", IntegerType(), True),\n",
    "    StructField(\"job ID\", IntegerType(), True),\n",
    "    StructField(\"task index\", IntegerType(), True),\n",
    "    StructField(\"machine ID\", IntegerType(), True),\n",
    "    StructField(\"event type\", IntegerType(), True),\n",
    "    StructField(\"user name\", StringType(), True),\n",
    "    StructField(\"scheduling class\", IntegerType(), True),\n",
    "    StructField(\"priority\", IntegerType(), True),\n",
    "    StructField(\"resource request for CPU cores\", FloatType(), True),\n",
    "    StructField(\"resource request for RAM\", FloatType(), True),\n",
    "    StructField(\"resource request for local disk space\", FloatType(), True),\n",
    "    StructField(\"different-machine constraint\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "# Charger le fichier CSV\n",
    "df_task_events = spark.read \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .schema(task_events_schema) \\\n",
    "    .csv(csv_directory_task_events)\n",
    "\n",
    "\n",
    "# Nettoyer les données en supprimant les entrées avec job ID null\n",
    "df_task_events_cleaned = df_task_events.filter(F.col(\"job ID\").isNotNull())\n",
    "df_task_events_cleaned.show()\n",
    "# Sélectionner les colonnes pertinentes\n",
    "df_task_events_cleaned_selected = df_task_events_cleaned.select(\n",
    "    \"job ID\", \"task index\", \"priority\")\n",
    "\n",
    "# LA PRIORITE EST LA MEME POUR TOUTES LES TASKS EVENTS ON ENLEVE LES DOUBLONS\n",
    "df_task_events_cleaned_selected = df_task_events_cleaned_selected.groupBy(\"job ID\", \"task index\").agg(\n",
    "    F.first(\"priority\").alias(\"priority\")\n",
    ")\n",
    "df_task_events_cleaned_selected.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jobs dominants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:==========================================>         (1152 + 32) / 1415]\r"
     ]
    }
   ],
   "source": [
    "# Ajouter une colonne pour la durée de chaque intervalle\n",
    "df_with_duration = df_task_usage_cleaned_selected.withColumn(\"duration\", F.col(\"end time\") - F.col(\"start time\"))#.cache() # Pour éviter à le recalculer, absent car trop volumineux\n",
    "\n",
    "# **Étape 2 : Analyse des tâches (Mémoire et CPU)**\n",
    "task_stats = df_with_duration.groupBy(\"job ID\", \"task index\") \\\n",
    "    .agg(\n",
    "        # Mémoire\n",
    "        F.max(\"canonical memory usage\").alias(\"max_memory_usage\"),\n",
    "        (F.sum(F.col(\"canonical memory usage\") * F.col(\"duration\")) / F.sum(\"duration\")).alias(\"avg_memory_usage\"),\n",
    "        # CPU\n",
    "        F.max(\"sampled CPU usage\").alias(\"max_cpu_usage\"),\n",
    "        (F.sum(F.col(\"sampled CPU usage\") * F.col(\"duration\")) / F.sum(\"duration\")).alias(\"avg_cpu_usage\"),\n",
    "    )\n",
    "\n",
    "# Calculer la valeur combinée pour la mémoire et le CPU (pour prendre en compte à la fois le max et l'average, 0.5 arbitraire)\n",
    "task_stats_combined = task_stats.withColumn(\n",
    "    \"combined_memory_usage\", \n",
    "    (F.col(\"max_memory_usage\") + F.col(\"avg_memory_usage\")) / 2\n",
    ").withColumn(\n",
    "    \"combined_cpu_usage\", \n",
    "    (F.col(\"max_cpu_usage\") + F.col(\"avg_cpu_usage\")) / 2\n",
    ").cache() # Pour éviter à le recalculer\n",
    "\n",
    "print(f\"Nombre total de lignes combinées (task_stats_combined) : {task_stats_combined.count()}\") #953\n",
    "\n",
    "\n",
    "# JOIN pour rajouter priority depuis df_task_events_cleaned_selected\n",
    "task_stats_combined_joined = task_stats_combined.join(\n",
    "    df_task_events_cleaned_selected,\n",
    "    on=[\"job ID\", \"task index\"],\n",
    ").cache() #IMPORTANT CETTE OPERATION PREND BEAUCOUP DE TEMPS ET NE RAJOUTE PAS DE LIGNES #953\n",
    "\n",
    "print(f\"Nombre total de lignes combinées (task_stats_combined_joined) : {task_stats_combined_joined.count()}\") #953\n",
    "\n",
    "# Trier et récupérer les tâches les plus prenantes (100 en mémoire et 100 en CPU)\n",
    "top_tasks_combined_joined_memory = task_stats_combined_joined.orderBy(F.desc(\"combined_memory_usage\"))\n",
    "top_tasks_combined_joined_cpu = task_stats_combined_joined.orderBy(F.desc(\"combined_cpu_usage\"))\n",
    "\n",
    "# Compter le nombre de lignes pour chaque fichier à sauvegarder\n",
    "count_tasks_memory = top_tasks_combined_joined_memory.count()\n",
    "count_tasks_cpu = top_tasks_combined_joined_cpu.count()\n",
    "\n",
    "# Afficher les résultats\n",
    "print(f\"Nombre de lignes à sauvegarder dans tasks_dominants_memory : {count_tasks_memory}\")\n",
    "print(f\"Nombre de lignes à sauvegarder dans tasks_dominants_cpu : {count_tasks_cpu}\")\n",
    "\n",
    "# Sauvegarder les résultats des tâches les plus prenantes\n",
    "tasks_output_directory_memory = \"output/usage/tasks_dominants_memory\"\n",
    "tasks_output_directory_cpu = \"output/usage/tasks_dominants_cpu\"\n",
    "top_tasks_combined_joined_memory.write.csv(tasks_output_directory_memory, header=True, mode='overwrite')\n",
    "top_tasks_combined_joined_cpu.write.csv(tasks_output_directory_cpu, header=True, mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrêter la session Spark après traitement\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialiser une session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Analyse Job Resource Usage\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Définition du schéma pour les fichiers CSV\n",
    "job_usage_schema = StructType([\n",
    "    StructField(\"job ID\", IntegerType(), True),                 # Identifiant unique du job\n",
    "    StructField(\"task index\", IntegerType(), True),             # Identifiant unique de la task du job\n",
    "    StructField(\"max_memory_usage\", FloatType(), True),         # Mémoire maximale utilisée\n",
    "    StructField(\"avg_memory_usage\", FloatType(), True),         # Mémoire moyenne utilisée\n",
    "    StructField(\"max_cpu_usage\", FloatType(), True),            # CPU maximal utilisé\n",
    "    StructField(\"avg_cpu_usage\", FloatType(), True),            # CPU moyen utilisé\n",
    "    StructField(\"combined_memory_usage\", FloatType(), True),    # Mémoire pondéré utilisé\n",
    "    StructField(\"combined_cpu_usage\", FloatType(), True),       # CPU pondéré utilisé\n",
    "    StructField(\"priority\", IntegerType(), True),               # priorité de la task\n",
    "])\n",
    "\n",
    "# Chemin vers le fichier CSV\n",
    "csv_file = \"output/usage/tasks_dominants_cpu/*.csv\"\n",
    "\n",
    "# Lire le fichier CSV en un DataFrame avec le schéma défini\n",
    "df_dominant_jobs = spark.read.csv(csv_file, header=True, schema=job_usage_schema)\n",
    "\n",
    "# Créer une liste des seuils pour lesquels filtrer les jobs\n",
    "thresholds = [k / 100 for k in range(0, 21, 1)]\n",
    "\n",
    "# Dictionnaire pour stocker les DataFrames filtrés\n",
    "filtered_dfs = {}\n",
    "\n",
    "# Liste pour stocker les corrélations et les comptages\n",
    "correlations = []\n",
    "counts = []\n",
    "\n",
    "# Filtrer les DataFrames pour chaque seuil\n",
    "for threshold in thresholds:\n",
    "    filtered_dfs[threshold] = df_dominant_jobs.filter(df_dominant_jobs[\"combined_cpu_usage\"] >= threshold)\n",
    "    count = filtered_dfs[threshold].count()\n",
    "    counts.append(count)\n",
    "    correlation = filtered_dfs[threshold].stat.corr(\"combined_memory_usage\", \"combined_cpu_usage\")\n",
    "    correlations.append((threshold, correlation))\n",
    "    print(f\"Threshold: {threshold}, Count: {count}, Corrélation: {correlation}\")\n",
    "\n",
    "# Tracer les résultats des corrélations\n",
    "threshold_values = [t[0] for t in correlations]\n",
    "correlation_values = [t[1] for t in correlations]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(threshold_values, correlation_values, marker='o', linestyle='-', color='b')\n",
    "plt.title(\"Corrélation entre combined_memory_usage et combined_cpu_usage\")\n",
    "plt.xlabel(\"Threshold (combined_cpu_usage)\")\n",
    "plt.ylabel(\"Corrélation\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Tracer les résultats des comptages\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(thresholds, counts, marker='o', linestyle='-', color='r')\n",
    "plt.title(\"Nombre de lignes après filtrage\")\n",
    "plt.xlabel(\"Threshold (combined_cpu_usage)\")\n",
    "plt.ylabel(\"Nombre de lignes\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypython",
   "language": "python",
   "name": "mypython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
